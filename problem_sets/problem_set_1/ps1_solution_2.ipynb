{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set #1: Supervised Learning\n",
    "[**Problem_set_1.pdf**](https://github.com/hsneto/stanford_cs229/blob/master/problem_sets/problem_set_1/ps1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "[Solution](https://github.com/hsneto/stanford_cs229/blob/master/problem_sets/problem_set_1/ps1_solution_1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Poisson regression and the exponential family\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a)\n",
    "**Notes:**\n",
    "\n",
    "$i)$ We say that a class of distributions is in the exponential family\n",
    "if it can be written in the form\n",
    "\n",
    "\\begin{align*}\n",
    "    p(y;\\eta)=b(y) \\exp(\\eta^TT(y)-a(\\eta))\n",
    "\\end{align*}\n",
    "\n",
    " - $\\eta$ is called the **natural parameter** (also called the **canonical parameter**) of the distribution; \n",
    " - $T(y)$ is the **sufficient statistic** (for the distributions we consider, it will often be the case that $T(y) = y$);\n",
    " - and $a(\\eta)$ is the **log partition function**.\n",
    " \n",
    "$ii)$ Consider the Poisson distribution parameterized by $\\lambda$:\n",
    "\n",
    "\\begin{align*}\n",
    "    p(y; \\lambda) = \\frac{e^{-\\lambda} \\lambda ^y}{y!}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the distribution function as:\n",
    "\n",
    "\\begin{align*}\n",
    "    p(y; \\lambda) \n",
    "        &= \\frac{e^{-\\lambda} e^{y \\log\\lambda}}{y!} \\\\\n",
    "        &= \\frac{1}{y!}\\exp(y\\log\\lambda - \\lambda)\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "    b(y) &= \\frac{1}{y!} \\\\\n",
    "    \\eta &= \\log\\lambda \\\\\n",
    "    T(y) &= y \\\\\n",
    "    a(\\eta) &= \\lambda = e^\\eta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b)\n",
    "The canonical response function for the GLM model will be\n",
    "\n",
    "\\begin{align*}\n",
    "    g(\\eta) &= \\mathrm{E}(T(y); \\eta) \\\\\n",
    "            &= \\mathrm{E}(y; \\eta) \\\\\n",
    "            &= \\lambda \\\\\n",
    "            &= e^{\\eta} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c)\n",
    "**Notes:**\n",
    "\n",
    "$i)$ _Assumption 3 from_ [note_1](https://github.com/hsneto/stanford_cs229/blob/master/course_materials/introduction/notes1.pdf):\n",
    " - The natural parameter $\\eta$ and the inputs $x$ are related linearly: $\\eta=\\theta^Tx$.\n",
    "\n",
    "$ii)$ **Stochastic gradient ascent rule:**\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta := \\theta + \\alpha\\nabla_\\theta\\ell(\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood of an example $(x^{(i)}, y^{(i)})$ is defined as $\\ell(\\theta) = \\log p(y^{(i)}\\ |\\ x^{(i)};\\theta)$.\n",
    "\n",
    "Hence, \n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla_\\theta\\ell(\\theta) = \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_j} \n",
    "        &= \\frac{\\partial}{\\partial \\theta_j}\\log\\Bigg(\\frac{1}{y^{(i)}!}\\exp(y^{(i)}\\log\\lambda - \\lambda)\\Bigg) \\\\\n",
    "        &= \\frac{\\partial}{\\partial \\theta_j}\\log\\Bigg(\\frac{1}{y^{(i)}!}\\Bigg)+\\frac{\\partial}{\\partial \\theta_j}\\Bigg(y^{(i)}\\log\\lambda - \\lambda\\Bigg) \\\\\n",
    "        &= \\frac{\\partial}{\\partial \\theta_j}\\log\\Bigg(\\frac{1}{y^{(i)}!}\\Bigg)+\\frac{\\partial}{\\partial \\theta_j}\\Bigg(y^{(i)}\\eta - e^\\eta\\Bigg)\\\\\n",
    "        &= \\frac{\\partial}{\\partial \\theta_j}\\log\\Bigg(\\frac{1}{y^{(i)}!}\\Bigg)+\\frac{\\partial}{\\partial \\theta_j}\\Bigg(y^{(i)}\\theta^Tx^{(i)} - e^{\\theta^Tx^{(i)}}\\Bigg) \\\\\n",
    "        &= y^{(i)}x_j^{(i)} - e^{\\theta^Tx^{(i)}}x_j^{(i)} \\\\\n",
    "        &= (y^{(i)}-e^{\\theta^Tx^{(i)}})x_j^{(i)}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta := \\theta + \\alpha(y^{(i)}-e^{\\theta^Tx})x_j^{(i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d)\n",
    "**Notes:**\n",
    " \n",
    "$i)$ **GLM:**\n",
    "\n",
    "\\begin{align*}\n",
    "    p(y;\\eta) = b(y) \\mathrm{exp}(\\eta T(y) - a(\\eta))\n",
    "\\end{align*}\n",
    "\n",
    "$ii)$ _Assumption 3 from_ [note_1](https://github.com/hsneto/stanford_cs229/blob/master/course_materials/introduction/notes1.pdf):\n",
    " - The natural parameter $\\eta$ and the inputs $x$ are related linearly: $\\eta=\\theta^Tx$.\n",
    " - Hence, $\\frac{\\partial \\eta}{\\partial \\theta_j}=x_j$\n",
    "\n",
    "$iii)$ **Stochastic gradient ascent rule:**\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta := \\theta + \\alpha\\nabla_\\theta\\ell(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "$iv)$ Consider that $p(y|x; \\theta)$ is a probability distribution and must thus sum to 1.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\int_{y}p(y|x; \\theta) dy = 1 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the GLM equation and considering that $T(y)=y$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "    p(y;\\eta) = b(y) \\mathrm{exp}(y\\eta - a(\\eta)) \n",
    "\\end{align*}\n",
    "\n",
    "Thus, \n",
    "\n",
    "\\begin{align*}\n",
    "     \\log p(y;\\eta)\n",
    "        &= \\log\\bigg(b(y) \\mathrm{exp}(y\\eta - a(\\eta))\\bigg)\\\\\n",
    "        &= \\log(b(y)) + \\log\\bigg(\\mathrm{exp}(y\\eta - a(\\eta))\\bigg)\\\\\n",
    "        &= \\log(b(y)) + y\\eta - a(\\eta)\n",
    "\\end{align*}\n",
    "\n",
    "The derivative:\n",
    "\n",
    "\\begin{align*}\n",
    "     \\frac{\\partial}{\\partial \\theta_j}\\log p(y;\\eta)\n",
    "        &= \\frac{\\partial}{\\partial \\theta_j}\\log(b(y)) + \\frac{\\partial}{\\partial \\theta_j}(y\\eta) - \\frac{\\partial}{\\partial \\theta_j}a(\\eta)\\\\\n",
    "        &= 0 + y\\frac{\\partial \\eta}{\\partial \\theta_j} - \\frac{\\partial a(\\eta)}{\\partial \\eta}\\frac{\\partial \\eta}{\\partial \\theta_j}\\\\\n",
    "        &= yx_j - \\frac{\\partial a(\\eta)}{\\partial \\eta}x_j\\\\\n",
    "        &= -\\Big(\\frac{\\partial a(\\eta)}{\\partial \\eta}-y\\Big)x_j\n",
    "\\end{align*}\n",
    "\n",
    "which give us\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta_j := \\theta_j - \\alpha\\Big(\\frac{\\partial a(\\eta)}{\\partial \\eta}-y\\Big)x_j\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Thus, it only remains to show that $\\frac{\\partial a(\\eta)}{\\partial \\eta} = h(x) = E[y|x; \\theta]$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\int_{y}p(y;\\eta) dy = 1 \n",
    "\\end{align*}\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{align*}\n",
    "    \\int_{y} \\frac{d}{d \\eta} p(y;\\eta) dy = 0\n",
    "\\end{align*}\n",
    "\n",
    "Transforming the left side,\n",
    "\n",
    "\\begin{align*}\n",
    "\\int_{y} \\frac{d}{d \\eta} p(y;\\eta) dy \n",
    "    &= \\int_{y} b(y) \\mathrm{exp}(\\eta y - a(\\eta))(y - \\frac{d a(\\eta)}{d \\eta}) dy \\\\\n",
    "    &= \\int_{y} p(y; \\eta)(y - \\frac{d a(\\eta)}{d \\eta}) dy \\\\\n",
    "    &= \\int_{y} p(y; \\eta) y - \\int_{y} p(y;\\eta) \\frac{d a(\\eta)}{d \\eta} dy \\\\\n",
    "    &= h(x) - \\frac{d a(\\eta)}{d \\eta} \\int_{y}p dy \\\\\n",
    "    &= h(x) - \\frac{d a(\\eta)}{d \\eta} 1 \\\\\n",
    "    &= h(x) - \\frac{d a(\\eta)}{d \\eta} \\\\\n",
    "    &= 0\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial a(\\eta)}{\\partial \\eta} = h(x)\n",
    "\\end{align*}\n",
    "\n",
    "So the stochastic gradient ascent rule is given as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta_j \n",
    "        &:= \\theta_j - \\alpha\\Big(\\frac{\\partial a(\\eta)}{\\partial \\eta}-y\\Big)x_j \\\\\n",
    "        &:= \\theta_j - \\alpha\\Big(h(x)-y\\Big)x_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For any GLM with a response variable from any member of the exponential family in which T(y) = y, and the canonical response function h(x) for the family. We show that stochastic gradient ascent on the log-likelihood log $p(y|X; \\theta)$ results in the update rule $\\theta_j:=\\theta_j - \\alpha\\Big(h(x)-y\\Big)x_j$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian discriminant analysis\n",
    "[Solution](https://github.com/hsneto/stanford_cs229/blob/master/problem_sets/problem_set_1/ps1_solution_3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Linear invariance of optimization algorithms\n",
    "[Solution](https://github.com/hsneto/stanford_cs229/blob/master/problem_sets/problem_set_1/ps1_solution_4.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Regression for denoising quasar spectra\n",
    "[Solution](https://github.com/hsneto/stanford_cs229/blob/master/problem_sets/problem_set_1/ps1_solution_5.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
